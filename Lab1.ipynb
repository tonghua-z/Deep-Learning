{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBD = 0\n",
    "N_BATCH = 100\n",
    "N_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(X):\n",
    "    mean_X = X.mean(axis=0)\n",
    "    std_X = X.std(axis=0)\n",
    "    return (X - mean_X) / std_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    with open('Dataset/'+filename, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "        X = (dict[b\"data\"] / 255).T\n",
    "        y = dict[b\"labels\"]\n",
    "        Y = (np.eye(10)[y]).T\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(dim):\n",
    "    np.random.seed(233)\n",
    "    W = np.random.normal(0,0.01,(10, dim))\n",
    "    b = np.random.normal(0,0.01,(10, 1))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X,W,b):\n",
    "    results = np.dot(W,X)+b\n",
    "    softmax = np.exp(results) / np.sum(np.exp(results), axis=0)\n",
    "    return (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, W, b, lambd):\n",
    "        N = X.shape[1]\n",
    "\n",
    "        P = EvaluateClassifier(X, W, b)\n",
    "        cost = 1/N * - np.sum(Y*np.log(P)) + lambd * np.sum(W**2)\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(y, P):\n",
    "    preds = np.argmax(P,axis=0).T\n",
    "    diff = preds - y\n",
    "    num_correct = diff[diff == 0].shape[0]\n",
    "    num_tot = np.array(y).shape[0]\n",
    "    acc = num_correct / float(num_tot)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, P, W, lambd):\n",
    "        N = X.shape[1]\n",
    "        G = - (Y - P)\n",
    "        grad_W = 1 / N * G@X.T + 2 * lambd * W\n",
    "        grad_b = np.reshape(1 / N * G@np.ones(N), (Y.shape[0], 1))\n",
    "        return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNum(X, Y, P, W, b, lamda, h = 1e-6):\n",
    "    \"\"\" Converted from matlab code \"\"\"\n",
    "    no = W.shape[0]\n",
    "    d = X.shape[0]\n",
    "\n",
    "    grad_W = np.zeros(W.shape);\n",
    "    grad_b = np.zeros((no, 1));\n",
    "\n",
    "    c = ComputeCost(X, Y, W, b, lamda);\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] += h\n",
    "        c2 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "        grad_b[i] = (c2-c) / h\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.array(W)\n",
    "            W_try[i,j] += h\n",
    "            c2 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "            grad_W[i,j] = (c2-c) / h\n",
    "\n",
    "    return [grad_W, grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, P, W, b, lamda, h = 1e-6):\n",
    "    \"\"\" Converted from matlab code \"\"\"\n",
    "    no = W.shape[0]\n",
    "    d = X.shape[0]\n",
    "\n",
    "    grad_W = np.zeros(W.shape);\n",
    "    grad_b = np.zeros((no, 1));\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] -= h\n",
    "        c1 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] += h\n",
    "        c2 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "\n",
    "        grad_b[i] = (c2-c1) / (2*h)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.array(W)\n",
    "            W_try[i,j] -= h\n",
    "            c1 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "\n",
    "            W_try = np.array(W)\n",
    "            W_try[i,j] += h\n",
    "            c2 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "\n",
    "            grad_W[i,j] = (c2-c1) / (2*h)\n",
    "\n",
    "    return [grad_W, grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Separate_mini_batch(Xtrain,Ytrain):\n",
    "    Xbatches = []\n",
    "    Ybatches = []\n",
    "    for j in range(1,int(Xtrain.shape[1]/N_BATCH)+1):\n",
    "        start = (j-1)*N_BATCH\n",
    "        end = j*N_BATCH\n",
    "        Xbatch = Xtrain[:,start:end]\n",
    "        Ybatch = Ytrain[:,start:end]\n",
    "        Xbatches.append(Xbatch)\n",
    "        Ybatches.append(Ybatch)\n",
    "    return Xbatches,Ybatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(Eta):\n",
    "    X,Y,y = LoadBatch('data_batch_1')\n",
    "    X = Normalization(X)\n",
    "    W,b = initialization(X.shape[0])\n",
    "    Xtest, Ytest, ytest = LoadBatch('test_batch')\n",
    "    Xtest = Normalization(Xtest)\n",
    "    Xbatches, Ybatches = Separate_mini_batch(X,Y)\n",
    "    \n",
    "\n",
    "    costs_train = []\n",
    "    costs_validation = []\n",
    "\n",
    "    for epoch in range(0,N_EPOCHS):\n",
    "        for batch in range(0,len(Xbatches)):\n",
    "\n",
    "            P = EvaluateClassifier(Xbatches[batch],W,b)\n",
    "\n",
    "            gradW, gradB = ComputeGradients(Xbatches[batch],Ybatches[batch],P,W,LAMBD)\n",
    "\n",
    "            W = W - Eta*gradW\n",
    "            b = b - Eta*gradB\n",
    "        # Eta = Eta * 0.9\n",
    "        \n",
    "        P_validation = EvaluateClassifier(Xtest,W,b)\n",
    "        P_train = EvaluateClassifier(X,W,b)\n",
    "        \n",
    "        cost_train = ComputeCost(X,Y,W,b,LAMBD)\n",
    "        cost_validation = ComputeCost(Xtest,Ytest,W,b,LAMBD)\n",
    "        \n",
    "        acc_train = ComputeAccuracy(y,P_train)\n",
    "        acc_validation = ComputeAccuracy(ytest,P_validation)\n",
    "        \n",
    "        print(\"Epoch: %d\" %(epoch+1))\n",
    "        print(\"Training Cost: %f\" %cost_train)\n",
    "        print(\"Validation Cost: %f\" %cost_validation)\n",
    "        print(\"Training Accuracy: %f\" %acc_train)\n",
    "        print(\"Validation Accuracy: %f\" %acc_validation)\n",
    "        print(\"---------------------------------\")\n",
    "        \n",
    "        costs_train.append(cost_train)\n",
    "        costs_validation.append(cost_validation)\n",
    "\n",
    "    return costs_validation,costs_train,W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(X,Y,P,W,b,lambd):\n",
    "        gradW_an, gradb_an = ComputeGradients(\n",
    "            X[:, :2], Y[:, :2], P, W, lambd)\n",
    "        gradW_nu, gradb_nu = ComputeGradsNum(\n",
    "            X[:, :2], Y[:, :2], P, W, b, lambd)\n",
    "        gradW_nu_sl, gradb_nu_sl = ComputeGradsNumSlow(\n",
    "            X[:, :2], Y[:, :2], P, W, b, lambd)\n",
    "        np.testing.assert_almost_equal(gradW_an, gradW_nu, decimal=6)\n",
    "        np.testing.assert_almost_equal(gradb_an, gradb_nu, decimal=6)\n",
    "        \n",
    "        np.testing.assert_almost_equal(gradW_an, gradW_nu_sl, decimal=6)\n",
    "        np.testing.assert_almost_equal(gradb_an, gradb_nu_sl, decimal=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grad():\n",
    "    X,Y,y = LoadBatch('data_batch_1')\n",
    "    W,b = initialization(X.shape[0])\n",
    "    Xbatches, Ybatches = Separate_mini_batch(X,Y)\n",
    "    try:\n",
    "        for batch in range(0,len(Xbatches)):\n",
    "            P = EvaluateClassifier(Xbatches[batch][:, :2],W,b)\n",
    "            check_gradients(Xbatches[batch], Ybatches[batch], P, W, b, LAMBD)\n",
    "            \n",
    "    except:\n",
    "        print(\"Gradients can work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montage(W):\n",
    "    \"\"\" Display the image for each label in W \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(2,5)\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            im  = W[i*5+j,:].reshape(32,32,3, order='F')\n",
    "            sim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "            sim = sim.transpose(1,0,2)\n",
    "            ax[i][j].imshow(sim, interpolation='nearest')\n",
    "            ax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "            ax[i][j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(costs_train,costs_validation):\n",
    "    loss_train=[]\n",
    "    loss_validation=[]\n",
    "    for i in range(len(costs_train)):\n",
    "        loss_train.append(sum(costs_train[0:i+1])/(i+1))\n",
    "        loss_validation.append(sum(costs_validation[0:i+1])/(i+1))\n",
    "    return loss_train, loss_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(costs_train,costs_validation):\n",
    "    epochs_arr = np.arange(0, N_EPOCHS).tolist()\n",
    "\n",
    "    plt.plot(epochs_arr, costs_train, 'r-',label='training cost')\n",
    "    plt.plot(epochs_arr, costs_validation, 'g-',label='validation cost')\n",
    "    plt.legend(loc='upper center', shadow=True)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(costs_train,costs_validation):\n",
    "    epochs_arr = np.arange(0, N_EPOCHS).tolist()\n",
    "\n",
    "    plt.plot(epochs_arr, costs_train, 'r-',label='training loss')\n",
    "    plt.plot(epochs_arr, costs_validation, 'g-',label='validation loss')\n",
    "    plt.legend(loc='upper center', shadow=True)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_validation,costs_train,W,b = train_network(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost(costs_train,costs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, loss_validation = compute_loss(costs_train,costs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(loss_train, loss_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
